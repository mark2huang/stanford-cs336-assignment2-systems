{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **1.2.1 æ³¨æ„åŠ›æœºåˆ¶å®ç°çš„å¤šå°ºåº¦åŸºå‡†æµ‹è¯•**\n---\n\nè¯·ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œåœ¨ä¸åŒçš„å°ºåº¦ä¸‹å¯¹ä½ å®ç°çš„æ³¨æ„åŠ›æœºåˆ¶è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚è„šæœ¬åº”åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š\n\n*   **(a)** å°†æ‰¹å¤§å°ï¼ˆBatch Sizeï¼‰å›ºå®šä¸º 8ï¼Œä¸”ä¸ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå³ï¼šç§»é™¤â€œå¤´â€ç»´åº¦ï¼Œå°†å…¶çœ‹ä½œå•å¤´ï¼‰ã€‚\n*   **(b)** éå†ä»¥ä¸‹å‚æ•°çš„ç¬›å¡å°”ç§¯ï¼šå¤´åµŒå…¥ç»´åº¦ $d_{model}$ å–å€¼ä¸º `[16, 32, 64, 128]`ï¼Œåºåˆ—é•¿åº¦ï¼ˆSequence Lengthï¼‰å–å€¼ä¸º `[256, 1024, 4096, 8192, 16384]`ã€‚\n*   **(c)** åˆ›å»ºå¯¹åº”å°ºå¯¸çš„éšæœºè¾“å…¥å¼ é‡ $Q, K, V$ã€‚\n*   **(d)** æµ‹é‡ä½¿ç”¨è¿™äº›è¾“å…¥è¿›è¡Œ 100 æ¬¡å‰å‘ä¼ æ’­ï¼ˆForward Passesï¼‰çš„æ—¶é—´ã€‚\n*   **(e)** åœ¨åå‘ä¼ æ’­å¼€å§‹å‰ï¼Œæµ‹é‡å½“å‰æ­£åœ¨ä½¿ç”¨çš„æ˜¾å­˜é‡ï¼ˆMemory in useï¼‰ï¼Œå¹¶æµ‹é‡ 100 æ¬¡åå‘ä¼ æ’­ï¼ˆBackward Passesï¼‰çš„æ—¶é—´ã€‚\n*   **(f)** ç¡®ä¿è¿›è¡Œé¢„çƒ­ï¼ˆWarm upï¼‰ï¼Œå¹¶ä¸”åœ¨æ¯æ¬¡å‰å‘/åå‘ä¼ æ’­ä¹‹åè°ƒç”¨ `torch.cuda.synchronize()`ï¼ˆä»¥ç¡®ä¿æµ‹é€Ÿå‡†ç¡®ï¼‰ã€‚\n\n**å®éªŒæŠ¥å‘Šè¦æ±‚ï¼š**\n\næ±‡æŠ¥åœ¨è¿™äº›é…ç½®ä¸‹å¾—åˆ°çš„è€—æ—¶æ•°æ®ï¼ˆå¦‚æœæŠ¥é”™åˆ™æ±‡æŠ¥æ˜¾å­˜æº¢å‡ºé”™è¯¯ï¼Œå³ Out-of-Memory, OOMï¼‰ã€‚\n1.  åœ¨ä»€ä¹ˆæ ·çš„å°ºå¯¸ä¸‹ä¼šå‡ºç°æ˜¾å­˜æº¢å‡ºï¼ˆOOMï¼‰ï¼Ÿ\n2.  é’ˆå¯¹ä½ å‘ç°çš„å‡ºç° OOM çš„æœ€å°é…ç½®ä¹‹ä¸€ï¼Œè¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶æ˜¾å­˜å ç”¨çš„â€œè´¦ç›®è®¡ç®—â€ï¼ˆä½ å¯ä»¥ä½¿ç”¨ Assignment 1 ä¸­å…³äº Transformer æ˜¾å­˜å ç”¨çš„è®¡ç®—å…¬å¼ï¼‰ã€‚\n3.  ä¸ºäº†åå‘ä¼ æ’­è€Œä¿å­˜ï¼ˆç¼“å­˜ï¼‰çš„æ˜¾å­˜é‡æ˜¯å¦‚ä½•éšåºåˆ—é•¿åº¦å˜åŒ–çš„ï¼Ÿ\n4.  ä½ ä¼šé‡‡å–ä»€ä¹ˆæ–¹æ³•æ¥æ¶ˆé™¤è¿™éƒ¨åˆ†æ˜¾å­˜å¼€é”€ï¼Ÿ\n\n**äº¤ä»˜ç‰©ï¼š**\nä¸€ä»½åŒ…å«è€—æ—¶æ•°æ®çš„è¡¨æ ¼ã€æ˜¾å­˜å ç”¨çš„è®¡ç®—æ¨å¯¼è¿‡ç¨‹ï¼Œä»¥åŠ 1-2 æ®µæ–‡å­—çš„å›ç­”ã€‚\n\n---","metadata":{}},{"cell_type":"code","source":"import os\nimport torch.nn.functional as F\n\ndef pytorch_naive_attention(q, k, v, mask=None):\n    \"\"\"\n    å®ç°æœ´ç´ çš„ PyTorch Attention: softmax(Q @ K.T / sqrt(dk)) @ V\n    \n    å‚æ•°:\n        q: Query å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)\n        k: Key å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)\n        v: Value å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)\n        mask: å¯é€‰çš„æ©ç å¼ é‡ (ä¾‹å¦‚ causal mask)\n        \n    è¿”å›:\n        output: æ³¨æ„åŠ›è®¡ç®—åçš„è¾“å‡ºï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)\n    \"\"\"\n    #step1:calculate attentionScore\n    attentionScore=q@k.transpose(-2,-1)\n\n    #step2:normalize\n    d_k=q.shape[-1]\n    attentionScore=attentionScore/d_k**0.5\n\n    #step3:mask\n    if mask is not None:\n        attentionScore=attentionScore.masked_fill(~mask,float('-inf'))\n\n    #step4:softmax\n    attentionScore=torch.softmax(attentionScore,dim=-1)\n\n    #step5:calculate output\n    output=attentionScore@v\n    return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T09:19:26.562637Z","iopub.execute_input":"2026-02-18T09:19:26.562969Z","iopub.status.idle":"2026-02-18T09:19:26.568868Z","shell.execute_reply.started":"2026-02-18T09:19:26.562941Z","shell.execute_reply":"2026-02-18T09:19:26.568133Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import time\nimport torch\nimport pandas as pd\nimport numpy as np\n\ndef benchmark_attention():\n    batch_size = 8\n    d_model_list = [16, 32, 64, 128]\n    seq_len_list = [256, 1024, 4096, 8192, 16384]\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    results = []\n\n    for d_model in d_model_list:\n        for seq_len in seq_len_list:\n            print(f\"Testing: d_model={d_model}, seq_len={seq_len}...\")\n            \n            # æ¸…ç†æ˜¾å­˜ç¼“å­˜\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n            \n            try:\n                # æ„é€ æ•°æ®ï¼Œéœ€è¦ requires_grad æ¥æµ‹è¯•åå‘ä¼ æ’­\n                q = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)\n                k = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)\n                v = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)\n\n                # --- 1. Warm up ---\n                for _ in range(10):\n                    _ = pytorch_naive_attention(q, k, v)\n\n                # --- 2. Measure Forward Pass ---\n                if device == \"cuda\": torch.cuda.synchronize()\n                start_fwd = time.perf_counter()\n                \n                for _ in range(100):\n                    out = pytorch_naive_attention(q, k, v)\n                \n                if device == \"cuda\": torch.cuda.synchronize()\n                end_fwd = time.perf_counter()\n                avg_fwd = (end_fwd - start_fwd) / 100 * 1000  # è½¬ä¸º ms\n\n                # --- 3. Measure Memory ---\n                # åœ¨åå‘ä¼ æ’­å¼€å§‹å‰è®°å½•æ˜¾å­˜\n                mem_usage = torch.cuda.memory_allocated(device) / (1024 ** 2) # MB\n\n                # --- 4. Measure Backward Pass ---\n                grad_output = torch.randn_like(out)\n                if device == \"cuda\": torch.cuda.synchronize()\n                start_bwd = time.perf_counter()\n                \n                for _ in range(100):\n                    out.backward(grad_output, retain_graph=True)\n                \n                if device == \"cuda\": torch.cuda.synchronize()\n                end_bwd = time.perf_counter()\n                avg_bwd = (end_bwd - start_bwd) / 100 * 1000 # ms\n\n                results.append({\n                    \"d_model\": d_model,\n                    \"seq_len\": seq_len,\n                    \"fwd_ms\": f\"{avg_fwd:.3f}\",\n                    \"bwd_ms\": f\"{avg_bwd:.3f}\",\n                    \"mem_mb\": f\"{mem_usage:.2f}\"\n                })\n\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower():\n                    print(f\"OOM at seq_len={seq_len}\")\n                    results.append({\n                        \"d_model\": d_model,\n                        \"seq_len\": seq_len,\n                        \"fwd_ms\": \"OOM\",\n                        \"bwd_ms\": \"OOM\",\n                        \"mem_mb\": \"OOM\"\n                    })\n                else:\n                    raise e\n\n    # æ‰“å°ç»“æœè¡¨æ ¼ (ä½œä¸š 1.1.2 å»ºè®®ä½¿ç”¨ pandas æ‰“å°è¡¨æ ¼)\n    df = pd.DataFrame(results)\n    print(\"\\nBenchmark Results:\")\n    print(df.to_markdown()) # éœ€è¦ pip install tabulate\n\nif __name__ == \"__main__\":\n    benchmark_attention()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:30:34.129486Z","iopub.execute_input":"2026-02-18T02:30:34.129805Z","iopub.status.idle":"2026-02-18T02:32:43.226834Z","shell.execute_reply.started":"2026-02-18T02:30:34.129779Z","shell.execute_reply":"2026-02-18T02:32:43.226072Z"}},"outputs":[{"name":"stdout","text":"Testing: d_model=16, seq_len=256...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","output_type":"stream"},{"name":"stdout","text":"Testing: d_model=16, seq_len=1024...\nTesting: d_model=16, seq_len=4096...\nTesting: d_model=16, seq_len=8192...\nTesting: d_model=16, seq_len=16384...\nOOM at seq_len=16384\nTesting: d_model=32, seq_len=256...\nTesting: d_model=32, seq_len=1024...\nTesting: d_model=32, seq_len=4096...\nTesting: d_model=32, seq_len=8192...\nTesting: d_model=32, seq_len=16384...\nOOM at seq_len=16384\nTesting: d_model=64, seq_len=256...\nTesting: d_model=64, seq_len=1024...\nTesting: d_model=64, seq_len=4096...\nTesting: d_model=64, seq_len=8192...\nTesting: d_model=64, seq_len=16384...\nOOM at seq_len=16384\nTesting: d_model=128, seq_len=256...\nTesting: d_model=128, seq_len=1024...\nTesting: d_model=128, seq_len=4096...\nTesting: d_model=128, seq_len=8192...\nTesting: d_model=128, seq_len=16384...\nOOM at seq_len=16384\n\nBenchmark Results:\n|    |   d_model |   seq_len | fwd_ms   | bwd_ms   | mem_mb   |\n|---:|----------:|----------:|:---------|:---------|:---------|\n|  0 |        16 |       256 | 0.118    | 6.040    | 10.62    |\n|  1 |        16 |      1024 | 1.241    | 2.177    | 50.38    |\n|  2 |        16 |      4096 | 15.336   | 29.282   | 536.75   |\n|  3 |        16 |      8192 | 64.601   | 120.728  | 2082.25  |\n|  4 |        16 |     16384 | OOM      | OOM      | OOM      |\n|  5 |        32 |       256 | 0.102    | 0.431    | 23.25    |\n|  6 |        32 |      1024 | 1.241    | 2.184    | 52.50    |\n|  7 |        32 |      4096 | 16.176   | 30.388   | 545.25   |\n|  8 |        32 |      8192 | 67.603   | 128.251  | 2100.25  |\n|  9 |        32 |     16384 | OOM      | OOM      | OOM      |\n| 10 |        64 |       256 | 0.108    | 0.307    | 28.25    |\n| 11 |        64 |      1024 | 1.306    | 2.512    | 56.75    |\n| 12 |        64 |      4096 | 19.095   | 34.864   | 562.25   |\n| 13 |        64 |      8192 | 81.289   | 152.397  | 2136.25  |\n| 14 |        64 |     16384 | OOM      | OOM      | OOM      |\n| 15 |       128 |       256 | 0.201    | 0.423    | 38.25    |\n| 16 |       128 |      1024 | 1.807    | 3.657    | 65.25    |\n| 17 |       128 |      4096 | 27.715   | 52.034   | 596.25   |\n| 18 |       128 |      8192 | 127.961  | 238.462  | 2208.25  |\n| 19 |       128 |     16384 | OOM      | OOM      | OOM      |\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"\nBenchmark Results:\n|    |   d_model |   seq_len | fwd_ms   | bwd_ms   | mem_mb   |\n|---:|----------:|----------:|:---------|:---------|:---------|\n|  0 |        16 |       256 | 0.118    | 6.040    | 10.62    |\n|  1 |        16 |      1024 | 1.241    | 2.177    | 50.38    |\n|  2 |        16 |      4096 | 15.336   | 29.282   | 536.75   |\n|  3 |        16 |      8192 | 64.601   | 120.728  | 2082.25  |\n|  4 |        16 |     16384 | OOM      | OOM      | OOM      |\n|  5 |        32 |       256 | 0.102    | 0.431    | 23.25    |\n|  6 |        32 |      1024 | 1.241    | 2.184    | 52.50    |\n|  7 |        32 |      4096 | 16.176   | 30.388   | 545.25   |\n|  8 |        32 |      8192 | 67.603   | 128.251  | 2100.25  |\n|  9 |        32 |     16384 | OOM      | OOM      | OOM      |\n| 10 |        64 |       256 | 0.108    | 0.307    | 28.25    |\n| 11 |        64 |      1024 | 1.306    | 2.512    | 56.75    |\n| 12 |        64 |      4096 | 19.095   | 34.864   | 562.25   |\n| 13 |        64 |      8192 | 81.289   | 152.397  | 2136.25  |\n| 14 |        64 |     16384 | OOM      | OOM      | OOM      |\n| 15 |       128 |       256 | 0.201    | 0.423    | 38.25    |\n| 16 |       128 |      1024 | 1.807    | 3.657    | 65.25    |\n| 17 |       128 |      4096 | 27.715   | 52.034   | 596.25   |\n| 18 |       128 |      8192 | 127.961  | 238.462  | 2208.25  |\n| 19 |       128 |     16384 | OOM      | OOM      | OOM      |","metadata":{}},{"cell_type":"markdown","source":"# å®éªŒæŠ¥å‘Šè¦æ±‚ï¼š\n\næ±‡æŠ¥åœ¨è¿™äº›é…ç½®ä¸‹å¾—åˆ°çš„è€—æ—¶æ•°æ®ï¼ˆå¦‚æœæŠ¥é”™åˆ™æ±‡æŠ¥æ˜¾å­˜æº¢å‡ºé”™è¯¯ï¼Œå³ Out-of-Memory, OOMï¼‰ã€‚\n\n## Q1ï¼šåœ¨ä»€ä¹ˆæ ·çš„å°ºå¯¸ä¸‹ä¼šå‡ºç°æ˜¾å­˜æº¢å‡ºï¼ˆOOMï¼‰ï¼Ÿ\né’ˆå¯¹ä½ å‘ç°çš„å‡ºç° OOM çš„æœ€å°é…ç½®ä¹‹ä¸€ï¼Œè¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶æ˜¾å­˜å ç”¨çš„â€œè´¦ç›®è®¡ç®—â€ï¼ˆä½ å¯ä»¥ä½¿ç”¨ Assignment 1 ä¸­å…³äº Transformer æ˜¾å­˜å ç”¨çš„è®¡ç®—å…¬å¼ï¼‰ã€‚\nA:å½“seq_len=16384çš„æ—¶å€™ï¼Œä¸ç®¡d_modelå¤§å°ï¼Œå†…å­˜éƒ½ä¼šæº¢å‡º\n\nQ,K,V Matrix=batchSize*d_model*seq_len*3*4byte=8*16*16384*3*4=0.02GB\n\nForward:attentionScore=batchsize*seq_len*seq_len*4byte=8*16384*16384*4=8GB\n\nBackward:ä¿ç•™å‰å‘çš„çŸ©é˜µ8GB\n\ntotal=QKVMatrix+Forward+Backward>16GBï¼ˆGPU T4*2ï¼‰\n","metadata":{}},{"cell_type":"markdown","source":"# 1.3.0\n\n---\n\n### **é—®é¢˜ (torch_compile)ï¼š2åˆ†**\n\n**(a)** æ‰©å±•ä½ ä¹‹å‰çš„æ³¨æ„åŠ›æœºåˆ¶åŸºå‡†æµ‹è¯•è„šæœ¬ï¼ŒåŠ å…¥ä½ æ‰€å®ç°çš„ PyTorch æ³¨æ„åŠ›æœºåˆ¶çš„**ç¼–è¯‘ç‰ˆæœ¬ï¼ˆcompiled versionï¼‰**ã€‚åœ¨ä¸ä¸Šè¿° `pytorch_attention` é—®é¢˜ï¼ˆå³ 1.2.1 èŠ‚ï¼‰ç›¸åŒçš„é…ç½®ä¸‹ï¼Œå¯¹æ¯”å…¶ä¸éç¼–è¯‘ç‰ˆæœ¬çš„æ€§èƒ½ã€‚\n\n*   **äº¤ä»˜ç‰©**ï¼šä¸€å¼ å¯¹æ¯”è¡¨æ ¼ï¼Œæ¯”è¾ƒç¼–è¯‘åçš„æ³¨æ„åŠ›æ¨¡å—ä¸ä¹‹å‰ `pytorch_attention` é—®é¢˜ä¸­éç¼–è¯‘ç‰ˆæœ¬çš„**å‰å‘ï¼ˆforwardï¼‰**å’Œ**åå‘ï¼ˆbackwardï¼‰**ä¼ æ’­è€—æ—¶ã€‚\n\n**(b)** ç°åœ¨ï¼Œåœ¨ä½ ä¹‹å‰çš„ç«¯åˆ°ç«¯åŸºå‡†æµ‹è¯•è„šæœ¬ï¼ˆå³ 1.1.3 èŠ‚ä¸­ç”¨äºæµ‹è¯•æ•´ä¸ªæ¨¡å‹çš„è„šæœ¬ï¼‰ä¸­ï¼Œ**ç¼–è¯‘ä½ çš„æ•´ä¸ª Transformer æ¨¡å‹**ã€‚å‰å‘ä¼ æ’­çš„æ€§èƒ½å‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–ï¼Ÿå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ä»¥åŠä¼˜åŒ–å™¨æ­¥éª¤ï¼ˆoptimizer stepsï¼‰ç»„åˆåœ¨ä¸€èµ·çš„æ•´ä½“æ€§èƒ½åˆå‘ç”Ÿäº†æ€æ ·çš„å˜åŒ–ï¼Ÿ\n\n*   **äº¤ä»˜ç‰©**ï¼šä¸€å¼ å¯¹æ¯”è¡¨æ ¼ï¼Œæ¯”è¾ƒ**åŸå§‹åŸç”Ÿï¼ˆvanillaï¼‰** Transformer æ¨¡å‹ä¸**ç¼–è¯‘åï¼ˆcompiledï¼‰** Transformer æ¨¡å‹çš„æ€§èƒ½ã€‚\n\n---\n\n### **ğŸ’¡ ä»»åŠ¡é‡ç‚¹è¯´æ˜ï¼š**\n\n#### **1. é’ˆå¯¹ (a) çš„å®ç°ï¼š**\nä½ ä¸éœ€è¦å†™æ–°å‡½æ•°ï¼Œåªéœ€è¦åˆ©ç”¨ `torch.compile`ã€‚\n*   **åšæ³•**ï¼šåœ¨æµ‹è¯•å¾ªç¯å¼€å§‹å‰ï¼Œæ‰§è¡Œ `compiled_fn = torch.compile(pytorch_naive_attention)`ã€‚\n*   **æ„ä¹‰**ï¼šä½ ä¼šè§‚å¯Ÿåˆ°ï¼Œç”±äº `torch.compile` å°è¯•å°†çŸ©é˜µä¹˜æ³•ã€é™¤æ³•ã€Maskã€Softmax è¿™äº›æ“ä½œâ€œèåˆâ€è¿›æ›´å°‘çš„ GPU å†…æ ¸ï¼ˆKernelsï¼‰ä¸­ï¼Œå‡å°‘äº†æ˜¾å­˜è¯»å†™æ¬¡æ•°ï¼Œé€Ÿåº¦åº”è¯¥ä¼šæœ‰æå‡ã€‚\n\n#### **2. é’ˆå¯¹ (b) çš„å®ç°ï¼š**\nè¿™æ¬¡æ˜¯é’ˆå¯¹**æ•´ä¸ªæ¨¡å‹ç±»**ã€‚\n*   **åšæ³•**ï¼š`compiled_model = torch.compile(model)`ã€‚\n*   **æµ‹é‡å†…å®¹**ï¼šè¿™æ¬¡ä¸ä»…ä»…æµ‹ Attention å‡½æ•°ï¼Œè¿˜è¦æµ‹é‡ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ­¥ï¼ˆTraining Stepï¼‰ï¼ŒåŒ…æ‹¬ï¼š\n    1.  `output = compiled_model(input)`ï¼ˆå‰å‘ï¼‰\n    2.  `loss.backward()`ï¼ˆåå‘ï¼‰\n    3.  `optimizer.step()`ï¼ˆä¼˜åŒ–å™¨æ›´æ–°ï¼‰\n*   **æ„ä¹‰**ï¼šå…¨æ¨¡å‹ç¼–è¯‘é€šå¸¸æ¯”å•å‡½æ•°ç¼–è¯‘æœ‰æ›´å¤§çš„ä¼˜åŒ–ç©ºé—´ï¼Œå› ä¸ºå®ƒèƒ½è·¨å±‚è¿›è¡Œç®—å­èåˆã€‚\n\n#### **3. âš ï¸ å®éªŒé™·é˜±æé†’ï¼š**\n*   **é¢„çƒ­ï¼ˆWarm-upï¼‰**ï¼š`torch.compile` åœ¨**ç¬¬ä¸€æ¬¡æ‰§è¡Œ**æ—¶ä¼šéå¸¸æ…¢ï¼Œå› ä¸ºå®ƒåœ¨åå°è°ƒç”¨ Triton ç”Ÿæˆå†…æ ¸ä»£ç ã€‚ä½ å¿…é¡»åœ¨è®¡æ—¶å¾ªç¯å¼€å§‹å‰ï¼Œå…ˆè·‘å‡ æ¬¡æ¨¡å‹ï¼Œç¡®ä¿ç¼–è¯‘å®Œæˆã€‚\n*   **æ˜¾å­˜**ï¼šæ³¨æ„è§‚å¯Ÿï¼Œè™½ç„¶é€Ÿåº¦å˜å¿«äº†ï¼Œä½†åœ¨é•¿åºåˆ—ï¼ˆå¦‚ 16384ï¼‰ä¸‹ï¼Œç¼–è¯‘ç‰ˆæ˜¯å¦ä¾ç„¶ä¼šæŠ¥é”™ OOMï¼Ÿï¼ˆç­”æ¡ˆé€šå¸¸æ˜¯ï¼šæ˜¯çš„ï¼Œå› ä¸ºå®ƒä¾ç„¶è¦å­˜ $N^2$ çš„æ³¨æ„åŠ›çŸ©é˜µï¼‰ã€‚\n","metadata":{}},{"cell_type":"code","source":"import time\nimport torch\nimport pandas as pd\nimport numpy as np\n\ndef benchmark_attention():\n    batch_size = 8\n    d_model_list = [16, 32, 64, 128]\n    seq_len_list = [256, 1024, 4096, 8192, 16384]\n    \n    if torch.cuda.is_available():\n        device = \"cuda\"\n        print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n        print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n        props = torch.cuda.get_device_properties(0)\n        print(f\"Total Memory: {props.total_memory / 1024**2:.2f} MB\")\n        print(f\"Processor Count: {props.multi_processor_count}\")\n        print(f\"Compute Capability: {props.major}.{props.minor}\")\n    else:\n        device =\"cpu\"\n    results = []\n\n    compiled_naive_attention = torch.compile(pytorch_naive_attention)\n    for d_model in d_model_list:\n        for seq_len in seq_len_list:\n            print(f\"Testing: d_model={d_model}, seq_len={seq_len}...\")\n            \n            # æ¸…ç†æ˜¾å­˜ç¼“å­˜\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n            \n            try:\n                # æ„é€ æ•°æ®ï¼Œéœ€è¦ requires_grad æ¥æµ‹è¯•åå‘ä¼ æ’­\n                q = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)\n                k = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)\n                v = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)\n\n                # --- 1. Warm up ---\n                for _ in range(10):\n                    warm_up_out = compiled_naive_attention(q, k, v)\n                    warm_up_out.backward(torch.randn_like(warm_up_out))\n                    \n                # --- 2. Measure Forward Pass ---\n                if device == \"cuda\": torch.cuda.synchronize()\n                start_fwd = time.perf_counter()\n                \n                for _ in range(100):\n                    out = compiled_naive_attention(q, k, v)\n                \n                if device == \"cuda\": torch.cuda.synchronize()\n                end_fwd = time.perf_counter()\n                avg_fwd = (end_fwd - start_fwd) / 100 * 1000  # è½¬ä¸º ms\n\n                # --- 3. Measure Memory ---\n                # åœ¨åå‘ä¼ æ’­å¼€å§‹å‰è®°å½•æ˜¾å­˜\n                mem_usage = torch.cuda.memory_allocated(device) / (1024 ** 2) # MB\n\n                # --- 4. Measure Backward Pass ---\n                grad_output = torch.randn_like(out)\n                if device == \"cuda\": torch.cuda.synchronize()\n                start = time.perf_counter()\n                \n                for _ in range(100):\n                    out=compiled_naive_attention(q,k,v)\n                    out.backward(grad_output)\n                if device == \"cuda\": torch.cuda.synchronize()\n                end = time.perf_counter()\n                avg_fwd_bwd = (end - start) / 100 * 1000 # ms æ³¨æ„è¿™é‡ŒåŒ…å«å‰å‘å’Œåå‘çš„æ—¶é—´\n                real_bwd_ms=avg_fwd_bwd-avg_fwd\n\n                results.append({\n                    \"d_model\": d_model,\n                    \"seq_len\": seq_len,\n                    \"fwd_ms\": f\"{avg_fwd:.3f}\",\n                    \"bwd_ms\": f\"{real_bwd_ms:.3f}\",\n                    \"mem_mb\": f\"{mem_usage:.2f}\"\n                })\n\n            except RuntimeError as e:\n                if \"out of memory\" in str(e).lower():\n                    print(f\"OOM at seq_len={seq_len}\")\n                    results.append({\n                        \"d_model\": d_model,\n                        \"seq_len\": seq_len,\n                        \"fwd_ms\": \"OOM\",\n                        \"bwd_ms\": \"OOM\",\n                        \"mem_mb\": \"OOM\"\n                    })\n                else:\n                    raise e\n\n    # æ‰“å°ç»“æœè¡¨æ ¼ (ä½œä¸š 1.3.1 å»ºè®®ä½¿ç”¨ pandas æ‰“å°è¡¨æ ¼)\n    df = pd.DataFrame(results)\n    print(\"\\nBenchmark Results:\")\n    print(df.to_markdown()) # éœ€è¦ pip install tabulate\n\nif __name__ == \"__main__\":\n    benchmark_attention()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T09:21:34.582034Z","iopub.execute_input":"2026-02-18T09:21:34.583106Z","iopub.status.idle":"2026-02-18T09:24:56.998324Z","shell.execute_reply.started":"2026-02-18T09:21:34.583077Z","shell.execute_reply":"2026-02-18T09:24:56.997533Z"}},"outputs":[{"name":"stdout","text":"Is CUDA available: True\nGPU Device Name: Tesla T4\nTotal Memory: 14912.69 MB\nProcessor Count: 40\nCompute Capability: 7.5\nTesting: d_model=16, seq_len=256...\nTesting: d_model=16, seq_len=1024...\nTesting: d_model=16, seq_len=4096...\nTesting: d_model=16, seq_len=8192...\nTesting: d_model=16, seq_len=16384...\nOOM at seq_len=16384\nTesting: d_model=32, seq_len=256...\nTesting: d_model=32, seq_len=1024...\nTesting: d_model=32, seq_len=4096...\nTesting: d_model=32, seq_len=8192...\nTesting: d_model=32, seq_len=16384...\nOOM at seq_len=16384\nTesting: d_model=64, seq_len=256...\nTesting: d_model=64, seq_len=1024...\nTesting: d_model=64, seq_len=4096...\nTesting: d_model=64, seq_len=8192...\nTesting: d_model=64, seq_len=16384...\nOOM at seq_len=16384\nTesting: d_model=128, seq_len=256...\nTesting: d_model=128, seq_len=1024...\nTesting: d_model=128, seq_len=4096...\nTesting: d_model=128, seq_len=8192...\nTesting: d_model=128, seq_len=16384...\nOOM at seq_len=16384\n\nBenchmark Results:\n|    |   d_model |   seq_len | fwd_ms   | bwd_ms   | mem_mb   |\n|---:|----------:|----------:|:---------|:---------|:---------|\n|  0 |        16 |       256 | 0.354    | 0.487    | 19.25    |\n|  1 |        16 |      1024 | 1.315    | 1.227    | 52.38    |\n|  2 |        16 |      4096 | 15.356   | 23.546   | 544.75   |\n|  3 |        16 |      8192 | 65.147   | 98.962   | 2098.25  |\n|  4 |        16 |     16384 | OOM      | OOM      | OOM      |\n|  5 |        32 |       256 | 0.939    | 0.860    | 24.25    |\n|  6 |        32 |      1024 | 3.713    | 4.169    | 56.50    |\n|  7 |        32 |      4096 | 23.691   | 32.629   | 561.25   |\n|  8 |        32 |      8192 | 80.472   | 124.365  | 2132.25  |\n|  9 |        32 |     16384 | OOM      | OOM      | OOM      |\n| 10 |        64 |       256 | 0.969    | 0.878    | 30.25    |\n| 11 |        64 |      1024 | 3.990    | 4.636    | 64.75    |\n| 12 |        64 |      4096 | 28.613   | 41.088   | 594.25   |\n| 13 |        64 |      8192 | 102.531  | 168.196  | 2200.25  |\n| 14 |        64 |     16384 | OOM      | OOM      | OOM      |\n| 15 |       128 |       256 | 1.091    | 0.925    | 42.25    |\n| 16 |       128 |      1024 | 4.598    | 5.939    | 81.25    |\n| 17 |       128 |      4096 | 40.429   | 60.686   | 660.25   |\n| 18 |       128 |      8192 | 147.319  | 250.266  | 2336.25  |\n| 19 |       128 |     16384 | OOM      | OOM      | OOM      |\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"- Is CUDA available: True\n- GPU Device Name: Tesla T4\n- Total Memory: 14912.69 MB\n- Processor Count: 40\n- Compute Capability: 7.5\n\nbefore complie:\n|    |   d_model |   seq_len | fwd_ms   | bwd_ms   | mem_mb   |\n|---:|----------:|----------:|:---------|:---------|:---------|\n|  0 |        16 |       256 | 0.118    | 6.040    | 10.62    |\n|  1 |        16 |      1024 | 1.241    | 2.177    | 50.38    |\n|  2 |        16 |      4096 | 15.336   | 29.282   | 536.75   |\n|  3 |        16 |      8192 | 64.601   | 120.728  | 2082.25  |\n|  4 |        16 |     16384 | OOM      | OOM      | OOM      |\n|  5 |        32 |       256 | 0.102    | 0.431    | 23.25    |\n|  6 |        32 |      1024 | 1.241    | 2.184    | 52.50    |\n|  7 |        32 |      4096 | 16.176   | 30.388   | 545.25   |\n|  8 |        32 |      8192 | 67.603   | 128.251  | 2100.25  |\n|  9 |        32 |     16384 | OOM      | OOM      | OOM      |\n| 10 |        64 |       256 | 0.108    | 0.307    | 28.25    |\n| 11 |        64 |      1024 | 1.306    | 2.512    | 56.75    |\n| 12 |        64 |      4096 | 19.095   | 34.864   | 562.25   |\n| 13 |        64 |      8192 | 81.289   | 152.397  | 2136.25  |\n| 14 |        64 |     16384 | OOM      | OOM      | OOM      |\n| 15 |       128 |       256 | 0.201    | 0.423    | 38.25    |\n| 16 |       128 |      1024 | 1.807    | 3.657    | 65.25    |\n| 17 |       128 |      4096 | 27.715   | 52.034   | 596.25   |\n| 18 |       128 |      8192 | 127.961  | 238.462  | 2208.25  |\n| 19 |       128 |     16384 | OOM      | OOM      | OOM      |\n\n\nafter compile:\n|    |   d_model |   seq_len | fwd_ms   | bwd_ms   | mem_mb   |\n|---:|----------:|----------:|:---------|:---------|:---------|\n|  0 |        16 |       256 | 0.354    | 0.487    | 19.25    |\n|  1 |        16 |      1024 | 1.315    | 1.227    | 52.38    |\n|  2 |        16 |      4096 | 15.356   | 23.546   | 544.75   |\n|  3 |        16 |      8192 | 65.147   | 98.962   | 2098.25  |\n|  4 |        16 |     16384 | OOM      | OOM      | OOM      |\n|  5 |        32 |       256 | 0.939    | 0.860    | 24.25    |\n|  6 |        32 |      1024 | 3.713    | 4.169    | 56.50    |\n|  7 |        32 |      4096 | 23.691   | 32.629   | 561.25   |\n|  8 |        32 |      8192 | 80.472   | 124.365  | 2132.25  |\n|  9 |        32 |     16384 | OOM      | OOM      | OOM      |\n| 10 |        64 |       256 | 0.969    | 0.878    | 30.25    |\n| 11 |        64 |      1024 | 3.990    | 4.636    | 64.75    |\n| 12 |        64 |      4096 | 28.613   | 41.088   | 594.25   |\n| 13 |        64 |      8192 | 102.531  | 168.196  | 2200.25  |\n| 14 |        64 |     16384 | OOM      | OOM      | OOM      |\n| 15 |       128 |       256 | 1.091    | 0.925    | 42.25    |\n| 16 |       128 |      1024 | 4.598    | 5.939    | 81.25    |\n| 17 |       128 |      4096 | 40.429   | 60.686   | 660.25   |\n| 18 |       128 |      8192 | 147.319  | 250.266  | 2336.25  |\n| 19 |       128 |     16384 | OOM      | OOM      | OOM      |","metadata":{"execution":{"iopub.status.busy":"2026-02-18T09:27:39.648436Z","iopub.execute_input":"2026-02-18T09:27:39.648818Z","iopub.status.idle":"2026-02-18T09:27:39.657034Z","shell.execute_reply.started":"2026-02-18T09:27:39.648790Z","shell.execute_reply":"2026-02-18T09:27:39.656004Z"}}},{"cell_type":"markdown","source":"### 1.3.1 Example - Weighted Sum (Triton å…¥é—¨)\n---\n**å®ƒæ˜¯åšä»€ä¹ˆçš„ï¼š**\nå¦‚ä½•ç”¨ Triton ç¼–å†™ä¸€ä¸ªç®€å•çš„å†…æ ¸ï¼šè®¡ç®—çŸ©é˜µ $X$ æ¯ä¸€è¡Œä¸å‘é‡ $w$ çš„ç‚¹ç§¯ã€‚\n\n**ä¸ºä»€ä¹ˆè¦å…ˆçœ‹è¿™ä¸ªï¼š**\n*   **ç†è§£â€œåˆ†å—æŒ‡é’ˆâ€ (Block Pointers)**ï¼šè¿™æ˜¯ Triton çš„æ ¸å¿ƒã€‚ä»¥å‰å†™ CUDA ç®—ç´¢å¼•å¾ˆç—›è‹¦ï¼ˆè¦ç®— `threadIdx`, `blockIdx` ç­‰ï¼‰ï¼ŒTriton ä½¿ç”¨ `tl.make_block_ptr` è®©ä½ åƒæ“ä½œåˆ‡ç‰‡ä¸€æ ·æ“ä½œ GPU å†…å­˜ã€‚\n*   **ç†è§£â€œæ­¥è¿›â€ (Advance)**ï¼šå­¦ä¹ å¦‚ä½•ç”¨ `ptr.advance` åœ¨å†…å­˜ä¸­ç§»åŠ¨ä½ çš„çª—å£ã€‚\n*   **ä»£ç å°è£…**ï¼šå­¦ä¹ å¦‚ä½•æŠŠ Triton å†…æ ¸åŒ…è£…è¿› `torch.autograd.Function` çš„ `forward` å’Œ `backward` ä¸­ï¼Œä½¿å…¶èƒ½æ— ç¼èå…¥ PyTorch çš„è®­ç»ƒæµç¨‹ã€‚\n\n**ä½ åº”è¯¥æ€ä¹ˆåšï¼š**\n1.  **ä¸è¦è·³è¿‡é˜…è¯»**ï¼šä»”ç»†é˜…è¯» PDF ç¬¬ 10-14 é¡µçš„ä»£ç ã€‚\n2.  **åœ¨ Kaggle æ‰‹æ•²ä¸€é**ï¼šå°è¯•è¿è¡Œ PDF ä¸­æä¾›çš„ `weighted_sum` ä»£ç ã€‚\n3.  **ç†è§£åå‘ä¼ æ’­**ï¼šçœ‹æ‡‚æ¢¯åº¦ $dw$ å’Œ $dx$ æ˜¯å¦‚ä½•åœ¨ Triton å¾ªç¯é‡Œç´¯åŠ çš„ã€‚\n\n---","metadata":{}},{"cell_type":"code","source":"#1.3.1 Example - Weighted Sum\ndef weighted_sum(x,weight):\n    # æ­¤å¤„å‡è®¾ x çš„ç»´åº¦æ˜¯ [..., D]ï¼Œweight çš„ç»´åº¦æ˜¯ [D],ç»´åº¦ä¸åŒ¹é…ï¼Œ å¹¿æ’­+ç‚¹ç§¯\n    return (weight *x).sum(axis=-1)\n\n\nimport triton\nimport triton.language as tl\n@triton.jit\ndef weight_sum_fwd(\n    x_ptr,weight_ptr, #è¾“å…¥æŒ‡é’ˆ\n    output_ptr, #è¾“å‡ºæŒ‡é’ˆ\n    x_row_stride,x_stride_dim,#æ­¥é•¿ï¼Œå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•åœ¨å¼ é‡çš„æ¯ä¸ªè½´ä¸Šç§»åŠ¨ä¸€ä¸ªå…ƒç´ \n    weight_stride_dim,#é€šå¸¸ä¸º1\n    output_stride_row,#é€šå¸¸ä¸º1\n    ROWS,D,#çŸ©é˜µæ€»è¡Œæ•°å’Œç»´åº¦\n    ROWS_TILE_SIZE:tl.constexpr,\n    D_TILE_SIZE: tl.constexpr,#åˆ†å—å¤§å°ï¼šå¿…é¡»åœ¨ç¼–è¯‘æ—¶å·²çŸ¥ å¯¹äº3*2çš„çŸ©é˜µROWS_TILE_SIZE=1ï¼ŒD_TILE_SIZE=2\n):\n    #æ¯ä¸ªå®ä¾‹å°†è®¡ç®—ä¸€ç»„è¡Œåˆ†å—çš„åŠ æƒå’Œ\n    # tl.program_id(0) æŒ‡ç¤ºå½“å‰åœ¨å“ªä¸ªçº¿ç¨‹å—\n    row_tile_idx=tl.program_id(0)\n    \n    #å—æŒ‡é’ˆï¼ˆBlock pointersï¼‰ è®©æˆ‘ä»¬èƒ½é€‰æ‹©å†…å­˜ä¸­çš„ä¸€ä¸ªNç»´åŒºåŸŸå¹¶ç§»åŠ¨å®ƒ\n    \"\"\"\n    å—æŒ‡é’ˆéœ€è¦çŸ¥é“ï¼š\n    - æŒ‡å‘å¼ é‡çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„æŒ‡é’ˆ\n    - å¼ é‡çš„æ€»å½¢çŠ¶ï¼ˆä»¥å¤„ç†è¶Šç•Œè®¿é—®ï¼‰\n    - æ¯ä¸ªç»´åº¦çš„æ­¥é•¿ï¼ˆä»¥æ­£ç¡®ä½¿ç”¨å†…å­˜å¸ƒå±€ï¼‰\n    - èµ·å§‹å—çš„Nç»´åæ ‡(offsets)\n    - æ¯æ¬¡åŠ è½½/å­˜å‚¨æ—¶ä½¿ç”¨çš„å—å½¢çŠ¶\n    - ç»´åº¦åœ¨å†…å­˜ä¸­çš„æ’åˆ—é¡ºåº\n    \"\"\"\n    x_block_ptr=tl.make_block_ptr(\n        x_ptr,\n        shape=(ROWS,D,),\n        strides=(x_row_stride,x_stride_dim),\n        offsets=(row_tile_idx * ROWS_TILE_SIZE,0),\n        block_shape=(ROWS_TILE_SIZE,D_TILE_SIZE),\n        order=(1,0),\n    )\n\n    weight_block_ptr=tl.make_block_ptr(\n        weight_ptr,\n        shape=(D,),\n        strides=(weight_stride_dim,),\n        offsets=(0,),\n        block_shape=(D_TILE_SIZE,),\n        orser=(0,),\n    )\n\n    output_block_ptr=tl.make_block_ptr(\n        output_ptr,\n        shape=(ROWS,),\n        strides=(output_stride_row,),\n        offsets=(row_tile_idx * ROWS_TILE_SIZE,),\n        block_shape=(ROWS_TILE_SIZE,),\n        orser=(0,),\n    )\n    #åˆå§‹åŒ–ä¸€ä¸ªç¼“å†²åŒºç”¨äºå†™å…¥ç»“æœ\n    output=tl.zeros((ROWS_TILE_SIZE,),dtype=tl.float32)\n    #åœ¨Dç»´åº¦ä¸Šè¿›è¡Œå¾ªç¯\n    for i in range(tl.cdiv(D,D_TILE_SIZE)):\n        #åŠ è½½å½“å‰çš„å—æŒ‡é’ˆå†…å®¹å¹¶å¸¦æœ‰è¾¹ç•Œæ£€æŸ¥\n        row=tl.load(x_block_ptr,boundary_check=(0,1),padding_option=\"zero\")\n        weight=tl.load(weight_block_ptr,boundary_check=(0,),padding_option=\"zero\")\n\n        #è®¡ç®—è¯¥è¡Œåˆ†å—çš„åŠ æƒå’Œ\n        output+=tl.sum(row*weight[None,:],axis=1)\n\n        #å°†æŒ‡é’ˆç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªåˆ†å—\n        x_block_ptr=x_block_ptr.advance((0,D_TILE_SIZE))\n        weight_block_ptr=weight_block_ptr.advance((D_TILE_SIZE,))\n    #å°†ç»“æœå†™å›è¾“å‡ºå¿«æŒ‡é’ˆ\n    tl.store(output_block_ptr,output,boundary_check=(0,))\n\nclass WeightedSumFunc(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx,x,weight):\n        #x=(Batch, Seq_len, D) such as (8, 512, 128)\n        D,output_dims=x.shape[-1],x.shape[:-1]\n        input_shape=x.shape\n        # æŠŠå‰é¢æ‰€æœ‰çš„ç»´åº¦ï¼ˆBatch, Seq_lenï¼‰å…¨éƒ¨å‹æ‰æˆä¸€ä¸ªç»´åº¦ã€‚(8, 512, 128) ä¼šå˜æˆ (4096, 128),è¿™æ ·æˆ‘ä»¬çš„ Triton å†…æ ¸åªéœ€è¦å¤„ç†ä¸€ä¸ª 2D çŸ©é˜µ å°±è¡Œäº†\n        x=rearrange(x,\"...d ->(...)d\")\n        \"\"\"\n        è¿™ä¸€æ­¥å°†å±•å¹³åçš„ x å’Œ weight ä¿å­˜åˆ° ctx å¯¹è±¡ä¸­ã€‚\n        å½“ä½ ä¹‹åè¿è¡Œ loss.backward() æ—¶,PyTorch ä¼šæŠŠè¿™ä¸¤ä¸ªå¼ é‡ä¼ ç»™ backward å‡½æ•°ï¼Œç”¨æ¥è®¡ç®—æ¢¯åº¦ã€‚\n        \"\"\"\n        ctx.save_for_backward(x,weight)\n\n        assert len(weight.shape)==1 and weight.shape[0]==D,\"Dimension mismatch\"\n        assert x.is_cuda and weight.is_cuda,\"Expected CUDA tensor\"\n        assert x.is_contiguous(),\"Our pointer arithmetic will assume contiguous x\" #is_contiguous (è¿ç»­æ€§)ï¼šè¿™æ˜¯æœ€å…³é”®çš„ï¼Triton çš„æŒ‡é’ˆç®—æ³•åŸºäºæ­¥é•¿ï¼ˆStrideï¼‰ã€‚å¦‚æœå¼ é‡åœ¨å†…å­˜é‡Œæ˜¯ä¸è¿ç»­çš„ï¼ˆæ¯”å¦‚ç»è¿‡äº†è½¬ç½®ä½†æ²¡è°ƒ contiguous()ï¼‰ï¼ŒæŒ‡é’ˆè®¡ç®—å°±ä¼šä¹±æ‰ï¼Œè¯»åˆ°é”™è¯¯çš„æ•°æ®ã€‚\n\n        ctx.D_TILE_SIZE=triton.next_power_of_2(D)#Triton çš„ block_shape å¿…é¡»æ˜¯ 2 çš„å¹‚,å¦‚æœD=128ï¼Œå°±ä¼šè®¡ç®—å‡º128/16=8\n        ctx.ROWS_TILE_SIZE=16\n        ctx.input_shape=input_shape\n\n        y=torch.empty(output_dims,device=x.device)\n\n        n_rows=y.numel()\n\n        \"\"\"\n        grid å®šä¹‰ï¼š(cdiv(n_rows, ctx.ROWS_TILE_SIZE), )ã€‚è¿™å†³å®šäº†å¯åŠ¨å¤šå°‘ä¸ªçº¿ç¨‹å—ã€‚å¦‚æœæ€»å…±æœ‰ 160 è¡Œï¼Œæ¯å—å¤„ç† 16 è¡Œï¼Œå°±å¯åŠ¨ 10 ä¸ªçº¿ç¨‹å—ã€‚\n        ä¼ é€’æ­¥é•¿ (stride)ï¼šæ‰‹åŠ¨æŠŠ PyTorch å¼ é‡çš„å†…å­˜æ­¥é•¿ä¼ ç»™å†…æ ¸ã€‚è¿™å°±æ˜¯æˆ‘ä»¬åœ¨å†…æ ¸é‡Œè®¡ç®—åœ°å€çš„ä¾æ®ã€‚\n        \"\"\"\n        weight_sum_fwd[(tl.cdiv(n_rows,ctx.ROWS_TILE_SIZE),)](\n            x.weight,\n            y,\n            x.stride(0),x.stride(1),\n            weight.stride(0),\n            y.stride(0),\n            ROWS=n_rows,D=D,\n            ROWS_TILE_SIZE=ctx.ROWS_TILE_SIZE,D_TILE_SIZE=ctx.D_TILE_SIZE,\n        )\n        return y.view(input_shape[:-1])#viewæŠŠå®ƒè¿˜åŸæˆåŸå§‹çš„å½¢çŠ¶ test\n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3.2 FlashAttention-2 Forward Pass (æ­£å¼æŒ‘æˆ˜)\nè¿™æ˜¯æ•´ä¸ªä½œä¸šä¸­åˆ†å€¼æœ€é«˜çš„éƒ¨åˆ†ä¹‹ä¸€ã€‚ç›®æ ‡æ˜¯å®ç° **Algorithm 1 (ç¬¬ 18 é¡µ)**ã€‚\n\nå®ƒåˆ†æˆäº†ä¸¤æ­¥èµ°ï¼š\n\n#### (a) ç¼–å†™çº¯ PyTorch çš„ Tiled Attention (Pure PyTorch, No Triton)\n*   **ä»»åŠ¡**ï¼šæŒ‰ç…§ Algorithm 1 çš„é€»è¾‘ï¼Œç”¨æ™®é€šçš„ Python `for` å¾ªç¯å’Œ PyTorch åˆ‡ç‰‡æ¥å®ç° Flash Attentionã€‚\n*   **æ ¸å¿ƒæŒ‘æˆ˜ï¼šOnline Softmax**ã€‚\n    *   æ ‡å‡†çš„ Softmax éœ€è¦çŸ¥é“ä¸€æ•´è¡Œçš„æœ€å¤§å€¼ã€‚\n    *   Flash Attention æ˜¯åˆ†å—ç®—çš„ã€‚å½“ä½ ç®—åˆ°ç¬¬ 2 å—æ—¶ï¼Œå‘ç°ç¬¬ 2 å—é‡Œæœ‰æ›´å¤§çš„å€¼ï¼Œä½ éœ€è¦ç”¨å…¬å¼å»**ä¿®æ­£**ï¼ˆRescaleï¼‰ç¬¬ 1 å—å·²ç»ç®—å¥½çš„éƒ¨åˆ†ç»“æœã€‚\n*   **å…¬å¼ï¼ˆçµé­‚ï¼‰**ï¼š\n    *   $m_i^{(new)} = \\max(m_i^{(old)}, \\text{rowmax}(S_i^{(j)}))$\n    *   $l_i^{(new)} = e^{m_i^{(old)} - m_i^{(new)}} \\cdot l_i^{(old)} + \\text{rowsum}(e^{S_i^{(j)} - m_i^{(new)}})$\n\n#### (b) ç¼–å†™ Triton å†…æ ¸ç‰ˆ\n*   **ä»»åŠ¡**ï¼šæŠŠä½ åˆšæ‰åœ¨ (a) ä¸­å†™é€šçš„é€»è¾‘ï¼Œç¿»è¯‘æˆ Triton è¯­æ³•ã€‚\n*   **ä¼˜åŠ¿**ï¼šTriton ç‰ˆä¼šæ¯” (a) å¿«å‡ ç™¾å€ï¼Œå› ä¸ºå®ƒæŠŠæ‰€æœ‰æ“ä½œï¼ˆçŸ©é˜µä¹˜æ³•ã€å½’ä¸€åŒ–ã€åŠ æƒæ±‚å’Œï¼‰å…¨éƒ¨èåˆåœ¨ä¸€ä¸ªå†…æ ¸é‡Œã€‚\n\n---","metadata":{}}]}